uv run --with mlx-lm -- pip install -Uqq mlx mlx_lm mlx_vlm Pillow

LOCAL_MODEL_PATH="./gemma-3n-e4b-it-mlx-bf16-local-git" # Or your path

uv run --with mlx-lm -- python -m mlx_lm.generate \
    --model "$LOCAL_MODEL_PATH" \
    --prompt "Write a short, whimsical story about a squirrel who becomes a renowned chef." \
    --max-tokens 500 \
    --temp 0.8 \
    --verbose \
    --device gpu # <--- Add this



# 1. Ensure Git LFS is installed and initialized
# If you used Homebrew:
brew install git-lfs
git lfs install

# If not using Homebrew, download from git-lfs.github.com and run installer, then `git lfs install`

# 2. Clone the model repository
# For Google's official Gemma 3n E4B-it:
echo "Cloning Google's official Gemma 3n E4B-it (7GB) via Git LFS..."
git clone https://huggingface.co/google/gemma-3n-E4B-it gemma-3n-e4b-it-google-local-git

# Or for the community-converted MLX bfloat16 version:
# echo "Cloning community-converted Gemma 3n E4B-it-MLX-bf16 (7GB) via Git LFS..."
# git clone https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-MLX-bf16 gemma-3n-e4b-it-mlx-bf16-local-git

echo "Download in progress. This may take some time depending on your internet speed."
echo "If interrupted, you can run 'cd gemma-3n-e4b-it-google-local-git && git pull' to resume."

# 3. Once cloned, run MLX from the local directory
# (After the clone completes)
# For text generation:
python -m mlx_lm.generate \
    --model ./gemma-3n-e4b-it-google-local-git \
    --prompt "Write a compelling speech about the future of AI and humanity's role in it." \
    --max-tokens 600 \
    --temp 0.7 \
    --verbose

# For multimodal generation (replace with your image path):
# python -m mlx_vlm.generate \
#     --model ./gemma-3n-e4b-it-google-local-git \
#     --prompt "Describe the historical context of this landmark." \
#     --image path/to/your/landmark.jpg \
#     --max-tokens 400 \
#     --temp 0.6 \
#     --verbose


from huggingface_hub import snapshot_download
import os

# Choose the model you want to download
# For Google's official Gemma 3n E4B-it:
repo_id = "google/gemma-3n-E4B-it"
local_dir_name = "gemma-3n-e4b-it-google-local"

# Or for the community-converted MLX bfloat16 version:
# repo_id = "lmstudio-community/gemma-3n-E4B-it-MLX-bf16"
# local_dir_name = "gemma-3n-e4b-it-mlx-bf16-local"

# Define where you want to save the model
local_dir_path = os.path.join(os.getcwd(), local_dir_name) # Saves in current directory

print(f"Starting download of {repo_id} to {local_dir_path}...")

try:
    snapshot_download(
        repo_id=repo_id,
        local_dir=local_dir_path,
        local_dir_use_symlinks=False, # Set to False for true copy, True for symlinks (saves disk space if using cache)
        resume_download=True          # Explicitly enables resume functionality (though often default)
    )
    print(f"Successfully downloaded {repo_id} to {local_dir_path}")
except Exception as e:
    print(f"An error occurred during download: {e}")
    print("Please check your internet connection and try again.")
